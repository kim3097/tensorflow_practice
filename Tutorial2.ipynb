{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 01 - Neural Network Basic\n",
    "# 털과 날개가 있는지 없는지에 따라, 포우류인지 조류인지 분류하는 신경망 모델을 만들어봅니다.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# [털, 날개]\n",
    "x_data = np.array(\n",
    "    [[0, 0], [1, 0], [1, 1], [0, 0], [0, 0], [0, 1]])\n",
    "\n",
    "# [기타, 포우류, 조류]\n",
    "# 다음과 같은 형식을 one-hot 형식의 데이터라고 합니다.\n",
    "y_data = np.array([\n",
    "        [1, 0, 0], # 기타\n",
    "        [0, 1, 0], # 포우류\n",
    "        [0, 0, 1], # 조류\n",
    "        [1, 0, 0],\n",
    "        [1, 0, 0],\n",
    "        [0, 0, 1],\n",
    "    ])\n",
    "\n",
    "###################\n",
    "# 신경망 모델 구성\n",
    "###################\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# 신경망은 2차원으로 [입력층(특성), 출력층(레이블)] -> [2, 3] 으로 정합니다.\n",
    "W = tf.Variable(tf.random_uniform([2, 3], -1., 1.))\n",
    "\n",
    "# 편향을 각각 각 레이어의 아웃풋 갯수로 설정합니다.\n",
    "# 편향은 아웃풋의 갯수, 즉 최종 결과값의 분류 갯수인 3으로 설정합니다.\n",
    "b = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# 신경망에 가중치 W과 편향 b을 적용합니다.\n",
    "L = tf.add(tf.matmul(X, W), b)\n",
    "# 가중치와 평향을 이용해 계산한 결과 값에\n",
    "# 텐서플로우에서 기본적으로 제공하는 활성화 함수인 ReLU 함수를 적용합니다.\n",
    "L = tf.nn.relu(L)\n",
    "\n",
    "# 마지막으로 softmax 함수를 이용하여 출력값을 사용하기 쉽게 만듭니다.\n",
    "# softmax 함수는 다음처럼 결과값을 전체합이 1인 확률로 만들어주는 함수입니다.\n",
    "# 예) [8.04, 2.76, -6.52] -> [0.53, 0.24, 0.23]\n",
    "model = tf.nn.softmax(L)\n",
    "\n",
    "# 신경망을 최적화하기 위한 비용 함수를 작성합니다.\n",
    "# 각 개별 결과에 대한 합을 구한 뒤 평균을 내는 방식을 사용합니다.\n",
    "# 전체 합이 아닌, 개별 결과를 구한 뒤 평균을 내는 방식을 사용하기 위해 axis 옵션을 사용합니다.\n",
    "# axis 옵션이 없으면 -1.09 처럼 총합인 스칼라값으로 출력됩니다.\n",
    "#        Y           model              Y * tf.log(model)    reduce_sum(axis=1)\n",
    "# 예)[[1 0 0]       [[0.1 0.7 0.2]   -> [[-1.0 0     0]    -> [-1.0 -0.09]\n",
    "#     [0 1 0]]       [0.2 0.8 0.0]       [0    -0.09 0]]\n",
    "# 즉, 이거은 예측값과 실제값 사이의 확률 분포의 차이를 비용으로 계산한것이며,\n",
    "# 이것을 Cross-Entropy 라고 합니다.\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(model), axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "##################\n",
    "# 신경망 모델 학습\n",
    "##################\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y: y_data})\n",
    "    \n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step + 1, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "        \n",
    "#####################\n",
    "# 결과 확인\n",
    "# 0 : 기타, 1: 포우류, 2: 조류\n",
    "#####################\n",
    "# tf.argmax: 예측값과 실제값의 행렬에서 tf.armax 를 이용해 가장 큰 값을 가져옵니다.\n",
    "# 예) [[0 1 0] [1 0 0]] -> [1 0]\n",
    "#     [[0.2 0.7 0.1] [0.9 0.1 0.]] -> [1 0]\n",
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y, 1)\n",
    "print(\"예측값:\", sess.run(prediction, feed_dict={X: x_data}))\n",
    "print(\"실제값:\", sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print(\"정확도: %.2f\" % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 02 - Deep NN.py\n",
    "# 털과 날개가 있는지 없는지에 따라, 포우류인지 조류인지 분류하는 신경망 모델을 만들어봅니다.\n",
    "# 신경망의 레이어를 여러개로 구서하여 말로만 듣던 딥러닝을 구성해 봅시다!\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# [털, 날개]\n",
    "x_data = np.array([[0, 0], [1, 0], [1, 1], [0, 0], [0, 0], [0, 1]])\n",
    "\n",
    "# [기타, 포우류, 조류]\n",
    "y_data = np.array([[1, 0, 0], # 기타\n",
    "                  [0, 1, 0], #포유류\n",
    "                  [0, 0, 1], # 조류\n",
    "                  [1, 0, 0],\n",
    "                  [1, 0, 0],\n",
    "                  [0, 0, 1]])\n",
    "\n",
    "##############\n",
    "# 신경망 모델 구성\n",
    "##############\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# 첫번째 가중치의 차원은 [특성, 히든 레이어의 뉴런갯수] -> [2, 10] 으로 정합니다.\n",
    "W1 = tf.Variable(tf.random_uniform([2, 10], -1, 1.))\n",
    "# 두번째 가중치의 차원을 [첫번째 히든 레이어의 뉴런갯수, 분류갯수] -> [10, 3]으로 정합니다.\n",
    "W2 = tf.Variable(tf.random_uniform([10, 3], -1, 1.))\n",
    "\n",
    "# 편향을 각가 각 레이어의 아웃풋 갯수로 설정합니다.\n",
    "# b1은 히든 레이어의 뉴런 갯수로, b2는 최종 결과값 즉, 분류 갯수인 3으로 설정합니다.\n",
    "b1 = tf.Variable(tf.zeros([10]))\n",
    "b2 = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# 신경망의 히든 레이어에 가중치 W1과 편향 b1을 적용합니다.\n",
    "L1 = tf.add(tf.matmul(X, W1), b1)\n",
    "L2 = tf.nn.relu(L1)\n",
    "\n",
    "# 최종적인 아웃풋을 계산합니다.\n",
    "# 히든레이어에 두번째 가중치 W2와 편향 b2를 적용하여 3개의 출력값을 만들어냅니다.\n",
    "model = tf.add(tf.matmul(L1, W2), b2)\n",
    "\n",
    "# 텐서플로우에서 기본적으로 제공하는 크로스 엔트로피 함수를 이용해\n",
    "# 복잡한 수식을 사용하지 않고도 최적화를 위한 비용 함수를 다음처럼 간단하게 적용할 수 있습니다.\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=model))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "#############################\n",
    "# 신경망 모델 학습\n",
    "#############################\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y: y_data})\n",
    "    \n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step + 1, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "        \n",
    "        \n",
    "############\n",
    "# 결과확인\n",
    "# 0: 기타, 0: 포유류, 1: 조류\n",
    "############\n",
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y, 1)\n",
    "print(\"예측값:\", sess.run(prediction, feed_dict={X: x_data}))\n",
    "print(\"실제값:\", sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print(\"정확도: %.2f\" % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 03 - Word2Vec.py\n",
    "# Word2Vec 모델을 간단하게 구현해봅시다.\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# matplot 에서 한글을 표시하기 위한 설정\n",
    "font_name = matplotlib.font_manager.FontProperties(fname=\"C:\\Windows\\Fonts\\gulim.ttc\"# 한글폰트\n",
    "                                                  ).get_name()\n",
    "matplotlib.rc('font', family=font_name)\n",
    "\n",
    "# 단어 벡터를 분석해볼 임의의 문장들\n",
    "sentences = [\"나 고양이 좋다\",\n",
    "            \"나 강아지 좋다\",\n",
    "            \"나 동물 좋다\",\n",
    "            \"강아지 고양이 동물\",\n",
    "            \"여자친구 고양이 강아지 좋다\",\n",
    "            \"고양이 생선 우유 좋다\",\n",
    "            \"강아지 생선 싫다 우유 좋다\",\n",
    "            \"강아지 고양이 눈 좋다\",\n",
    "            \"나 여자친구 좋다\",\n",
    "            \"여자친구 나 싫다\",\n",
    "            \"여자친구 나 영화 책 음악 좋다\",\n",
    "            \"나 게임 만화 애니 좋다\",\n",
    "            \"고양이 강아지 싫다\",\n",
    "            \"강아지 고양이 좋다\"]\n",
    "\n",
    "# 문장을 전부 합친 후 공백으로 단어들을 나누고 고유한 단어들로 리스트를 만듭니다.\n",
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "# 문자열로 분석하는 것 보다, 숫자로 분석하는 것이 훨씬 용이하므로\n",
    "# 리스트에서 문자들의 인덱스를 뽑아서 사용하기 위해,\n",
    "# 이를 포현하기 위한 연관 배열과, 단어 리스트에서 단어를 참조 할 수 있는 인덱스 배열을 만듭니다.\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "word_index = [word_dict[word] for word in word_list]\n",
    "\n",
    "# 윈도우 사이즈를 1로 하는 skip-gram 모델을 만듭니다.\n",
    "# 예) 나 게임 만화 애니 좋다\n",
    "#    -> ([나, 만화], 게임), ([게임, 애니], 만화), ([만화, 좋다], 애니)\n",
    "#    -> (게임, 나), (게임, 만화), (만화, 게임), (만화, 애니), (애니, 만화), (애니, 좋다)\n",
    "skip_grams = []\n",
    "\n",
    "for i in range(1, len(word_index) - 1):\n",
    "    # (context, target) : ([target index - 1, target index + 1], target)\n",
    "    target = word_index[i]\n",
    "    context = [word_index[i - 1], word_index[i + 1]]\n",
    "    \n",
    "    # (target, context[0]), (target, context[1])..\n",
    "    for w in context:\n",
    "        skip_grams.append([target, w])\n",
    "        \n",
    "# skip-grams 데이터에서 무작위로 데이터를 뽑아 입력값과 출력값의 배치 데이터를 생성하는 함수\n",
    "def random_batch(data, size):\n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(data)), size, replace=False)\n",
    "    \n",
    "    for i in random_index:\n",
    "        random_inputs.append(data[i][0]) # target\n",
    "        random_labels.append([data[i][1]]) # context word\n",
    "        \n",
    "    return random_inputs, random_labels\n",
    "\n",
    "##################\n",
    "# 옵션 설정\n",
    "##################\n",
    "# 학습을 반복할 횟수\n",
    "training_epoch = 300\n",
    "# 학습률\n",
    "learning_rate = 0.1\n",
    "# 한 번에 학습할 데이터의 크기 \n",
    "batch_size = 20\n",
    "# 단어 벡터를 구성할 임베딩 차원의 크기\n",
    "# 이 예제에서는 x, y 그래프로 표현하기 쉽게 2개의 값만 출력하도록 합니다.\n",
    "embedding_size = 2\n",
    "# word2vec 모델을 학습시키기 위한 nce_loss 함수에서 사용하기 위한 샘플링 크기\n",
    "# batch_size 보다 작아야 합니다.\n",
    "num_sampled = 15\n",
    "# 총 단어 갯수 \n",
    "voc_size = len(word_list)\n",
    "\n",
    "###############\n",
    "# 신경망 모델 구성\n",
    "###############\n",
    "inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "# tf.nn.nec_loss를 사용하려면 출력값을 이렇게 [batch_size, 1] 로 구성해야합니다.\n",
    "labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "# word2vec 모델의 결과 값인 임베딩 벡터를 저장할 변수입니다.\n",
    "# 총 단어 갯수와 임베딩 갯수를 크기로 하는 두 개의 차원을 갖습니다.\n",
    "embeddings = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
    "# 임베딩 벡터의 차원에서 학습할 입력값에 대한 행들을 뽑아옵니다.\n",
    "# 예) embeddinigs     inputs     selected\n",
    "#    [[1, 2, 3]    -> [2, 3]  -> [[2, 3, 4]\n",
    "#     [2, 3, 4]                   [3, 4, 5]]\n",
    "#     [3, 4, 5]\n",
    "#     [4, 5, 6]]\n",
    "selected_embed = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "\n",
    "# nce_loss 함수에서 사용할 변수들을 정의합니다.\n",
    "nce_weights = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
    "nce_biases = tf.Variable(tf.zeros([voc_size]))\n",
    "\n",
    "# nce_loss 함수를 직접 구현하려면 매우 복잡하지만,\n",
    "# 함수를 텐서플로우가 제공하므로 그냥 tf.nn.nce_loss 함수를 사용하기만 하면 됩니다.\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(nce_weights, nce_biases, labels, selected_embed, num_sampled, voc_size))\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "############\n",
    "# 신경망 모델 학습\n",
    "############\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    for step in range(1, training_epoch + 1):\n",
    "        batch_inputs, batch_labels = random_batch(skip_grams, batch_size)\n",
    "        \n",
    "        _, loss_val = sess.run([train_op, loss], feed_dict={inputs: batch_inputs,\n",
    "                                                           labels: batch_labels})\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print(\"loss at step \", step, \": \", loss_val)\n",
    "            \n",
    "        # matplot 으로 출력하여 시각적으로 확인해보기 위해\n",
    "        # 임베딩 벡터의 결과 값을 계산하여 저장합니다.\n",
    "        # with 구문 안에서는 sess.run 대신 간단히 eval() 함수를 사용할 수 있습니다.\n",
    "        trained_embeddings = embeddings.eval()\n",
    "        \n",
    "        \n",
    "#############\n",
    "# 임베딩된 Word2Vec 결과 확인\n",
    "# 결과는 해당 단어들이 얼마나 다른 단어와 인접해 있는지를 보여줍니다.\n",
    "############\n",
    "for i, label in enumerate(word_list):\n",
    "    x, y = trained_embeddings[i]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label, xy=(x, y), xytext=(5,2), textcoords='offset points', ha='right', va='bottom')\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
